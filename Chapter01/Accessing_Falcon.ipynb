{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b65d3c5-5afe-4ccd-b2f1-366fc9c0dfa3",
   "metadata": {},
   "source": [
    "## Accessing Falcon LLM: Jupyter Notebook Tutorial\n",
    "Falcon LLM is a powerful language model, and there are multiple ways to access and utilize it. In this tutorial, we will explore three primary methods to access the Falcon-1.3B LLM.\n",
    "\n",
    "#### Why are we downloading the 1.3B version of Falcon?\r\n",
    "\r\n",
    "You might be wondering why we've chosen to work with the 1.3B parameter version of Falcon, especially when there are larger and more popular versions available, such as the 7B, 40B, and 180B parameter models.\r\n",
    "\r\n",
    "The primary reason is simplicity and speed for the purpose of this tutorial. The 1.3B version is significantly smaller in size compared to its larger counterparts. This makes it easier and quicker to download, especially for those with limited bandwidth or slower internet connections.\r\n",
    "\r\n",
    "While the larger models might offer increased performance or capabilities in certain applications, our goal here is not to dive deep into model performance. Instead, we want to provide a clear and concise walkthrough of the access mechanics. Using the 1.3B version allows us to achieve this objective without overwhelming the user or causing unnecessary delays.\r\n",
    "\r\n",
    "In practice, once you're familiar with accessing the 1.3B model, you can apply the same mechanics to access any other model version as per your requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc117a-37ad-4458-a9cb-f52d93121e85",
   "metadata": {},
   "source": [
    "### 1. Load as Hugging Face Pipeline\r\n",
    "\r\n",
    "**Explanation**: The Hugging Face pipeline provides a high-level, easy-to-use API for several tasks, including text generation. When using the pipeline, much of the pre-processing and post-processing is abstracted away, allowing users to focus solely on their task.\r\n",
    "\r\n",
    "**Use Case**: This method is best suited for those who are looking for a quick way to utilize the model without the need for customization or fine-tuning. It's also ideal for beginners or those who prefer a simplified interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97056f6-ab1e-4b39-b40f-4802cdb7bab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"tiiuae/falcon-rw-1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3075b614-7f35-4d70-98aa-ae196456390b",
   "metadata": {},
   "source": [
    "### 2. Load Directly Using Hugging Face Transformers Library\r\n",
    "\r\n",
    "**Explanation**: Loading the model and tokenizer directly provides more flexibility compared to the pipeline method. With direct access to the tokenizer and model, users can customize tokenization options, pass additional arguments to the model, or use the model in a different context.\r\n",
    "\r\n",
    "**Use Case**: This approach is well-suited for advanced users or those who need to deeply integrate the Falcon LLM into their applications. It also provides a pathway to fine-tuning or training the model with custom datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a975dbfa-b886-4e34-97e0-be7e1404db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly using Hugging Face transformers library\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-rw-1b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-rw-1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f48bb-3549-4d9e-9faa-e6ce5143aa92",
   "metadata": {},
   "source": [
    "### 3. Download Model File from Another Source\r\n",
    "\r\n",
    "**Explanation**Sometimes, forto various reasons like working in an offline environmenor needinged to keep a local copy of the model, users might want to download the model file directly from a source URL. This method allows you to get the raw model weights anfiles.le In this example, we will download the pytorch model file for the same 1B Falcon mod.s.\r\n",
    "\r\n",
    "**Use Case**: This method is best for users who need to work in environments without direct internet access to Hugging Face's model hub or who want to keep a versioned copy of the model weights locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326543e-7695-4ead-8777-168b3e8d35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "url = \"https://huggingface.co/tiiuae/falcon-rw-1b/resolve/main/modeling_falcon.py\"\n",
    "model_path = \"./modeling_falcon.py\"\n",
    "\n",
    "# send a GET request to the URL to download the file. Stream since it's large\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "# open the file in binary mode and write the contents of the response to it in chunks\n",
    "# This is a large file, so be prepared to wait.\n",
    "with open(model_path, 'wb') as f:\n",
    "    for chunk in tqdm(response.iter_content(chunk_size=10000)):\n",
    "        if chunk:\n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b18dcf-142f-457e-88a5-f6c6845c60d7",
   "metadata": {},
   "source": [
    "Remember, the choice of method depends largely on your specific use case and requirements. The pipeline provides a quick and easy way to get started, while direct model loading and downloading offer more flexibility and control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84edec2f-dc2f-4c3d-bb04-df380a686b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tutorial complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

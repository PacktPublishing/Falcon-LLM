{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ae2eda-d32e-4a53-88d6-86aa7d619f76",
   "metadata": {},
   "source": [
    "# Fine-Tuning Falcon 1.3B Model for Binary Classification\n",
    "In this Jupyter Notebook, we will guide you through the process of fine-tuning a 1.3B parameter Falcon model for a binary classification task using the GLUE MRPC dataset. Our objective is to demonstrate how to adapt a large pre-trained model for a specific NLP task, which in our case is to determine if two sentences are semantically equivalent.\n",
    "\n",
    "## Learning Objectives:\n",
    "1. Learn how to load and adapt a large pre-trained language model using PyTorch and the Transformers library.\n",
    "2. Understand the process of creating a custom classification layer on top of the base model.\n",
    "3. Explore dataset preprocessing and preparation for model training.\n",
    "4. Implement the training and evaluation process for the model.\n",
    "5. Save the fine-tuned model for future inference tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2889f6-d552-437e-96d6-938d80aa0e6d",
   "metadata": {},
   "source": [
    "# Importing Required Libraries\r\n",
    "This cell imports essential libraries for our task. We include PyTorch for model building and training, the Transformers library for accessing pre-trained models and tokenizers, and the `datasets` library for loading the GLUE MRPC dataset.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9e81e24-8196-4cf0-a6a3-c3a99524b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import cross_entropy\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d94a29-1d14-4bf5-8ff4-417f4ef779c8",
   "metadata": {},
   "source": [
    "# Loading the Model and Tokenizer\r\n",
    "Here, we load the Falcon 1.3B model and its tokenizer. We also set the padding token to be the same as the EOS (end of sentence) token, which is a necessary configuration for some transformer models.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f9c29-1436-4d54-a1cc-75635c139200",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tiiuae/falcon-rw-1b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f4199-d32f-4dbe-bd18-cd3833dab0d0",
   "metadata": {},
   "source": [
    "# Defining a Custom Classifier\r\n",
    "In this cell, we define a custom classification layer, `CustomClassifier`, that adds a linear layer on top of the base Falcon model. This classifier will be responsible for binary classification (two labels). We also specify the forward pass operations for the model.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40afa8d5-a4e8-4b10-861b-de7de9f16b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.base_model.config.hidden_size, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get the outputs from the base model\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the last hidden state\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        # Use the hidden state of the first token (CLS token for BERT-like models)\n",
    "        cls_token_state = last_hidden_state[:, 0, :]  # Shape: [batch_size, hidden_size]\n",
    "\n",
    "        # Pass through the classifier\n",
    "        logits = self.classifier(cls_token_state)\n",
    "        return logits\n",
    "\n",
    "num_labels = 2  # Adjust as per your task\n",
    "model = CustomClassifier(base_model, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff41bf-1ee9-4bfb-88c2-81d1f55f0661",
   "metadata": {},
   "source": [
    "# Dataset Preparation\r\n",
    "We load the GLUE MRPC dataset and prepare a custom PyTorch Dataset, `GlueDataset`. This class handles tokenization and prepares the input data and labels for training and evaluation. We create two instances of this dataset for training and validation purposes.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1cc17-f6d9-408a-a485-f3cc4dbad051",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")  # Example dataset\n",
    "\n",
    "class GlueDataset(Dataset):\n",
    "    def __init__(self, tokenizer, raw_datasets, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "\n",
    "        for sentence1, sentence2, label in zip(raw_datasets['sentence1'], raw_datasets['sentence2'], raw_datasets['label']):\n",
    "            tokenized_input = self.tokenizer(sentence1, sentence2, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "            self.inputs.append(tokenized_input)\n",
    "            self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val.squeeze(0) for key, val in self.inputs[idx].items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = GlueDataset(tokenizer, raw_datasets['train'])\n",
    "eval_dataset = GlueDataset(tokenizer, raw_datasets['validation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0c57e-ef66-427d-b8ca-24d1e882d732",
   "metadata": {},
   "source": [
    "# Setting up DataLoaders\r\n",
    "This cell creates DataLoaders for both the training and evaluation datasets. DataLoaders are used to efficiently load data in batches, with shuffling enabled for the training data to improve model generalization.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71edc1d9-4eda-44ce-bc7e-cf91a82dfdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c5e60-acd5-48f3-9441-705f40430302",
   "metadata": {},
   "source": [
    "# Configuring the Model and Optimizer\r\n",
    "We set up the device (CPU or GPU) for training and move the model to the chosen device. Additionally, we initialize the optimizer, `AdamW`, which will update the model weights during training.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3539a14-01bc-43d8-bbe0-5e22ccf20e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d82834-e293-427c-a391-2e8987acab85",
   "metadata": {},
   "source": [
    "# Model Training Loop\n",
    "\n",
    "This cell contains the training loop for the model:\n",
    "- Iterates over epochs\n",
    "- Performs forward and backward passes\n",
    "- Updates model weights using the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7087393-6ce5-453d-8dc9-0a000bac9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c488d7-3bb0-47a0-8712-f7bf52d48122",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Here, the model is evaluated on the validation dataset. The model is set to evaluation mode `model.eval()` and we use our evaluation data loader to pass the eval data samples through the fine-tuned model. The accuracy is calculated and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4934d4-ea4a-4a09-887b-f456e71bc23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_eval_accuracy = 0\n",
    "for batch in eval_loader:\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        total_eval_accuracy += torch.sum(predictions == labels).item()\n",
    "\n",
    "avg_accuracy = total_eval_accuracy / len(eval_loader.dataset)\n",
    "print(f\"Accuracy on evaluation data: {avg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989bdb7-10a5-44f8-8cfb-184f32573564",
   "metadata": {},
   "source": [
    "# Saving the Model\n",
    "\n",
    "Finally, in this cell, the trained model's state is saved to a file for future use or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e56576-f1b1-4023-9ae8-adc4a318a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./my_custom_model_falcon.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
